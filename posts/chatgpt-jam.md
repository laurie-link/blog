---
title: ChatGPT相关黑话，再不知道就太晚了！
date: 2024-11-05
excerpt: 一些比较基本但也重要的关于ChatGPT的术语
tags: ["AI", "深度AI博文"]
category: AI学习
---

原文链接：


[Statics](https://statics.itc.cn)


2023-03-27 17:09 发布于：福建省 本来准备一篇文章搞定的，发现能讲的还挺多的。所以，先写这一篇，后面再续！都是很重要的概念！

请放心，这篇文章不是ChatGPT 生成的。是我一个字一个字敲进去的。

ChatGPT 一下子火起来。不过，对于还没有认真研究这个领域的做数字营销的朋友而言，很多术语扑面而来，理解起来太费劲。

毕竟，我们做数字营销，是ChatGPT 等AIGC技术的应用者，但不是开发者。可是，如果不了解重要概念，在应用时候就会难以真正理解背后的原理，就会被具体的功能牵着鼻子走，而难以有创新。

所以，下面这些“黑话”，以及背后的逻辑，我们有必要知道。

## GPT

GPT是“Generative Pre-trained Transformer”

（生成型预训练变换模型）的缩写，目的是为了使用深度学习生成人类可以理解的自然语言。

理解人类自然语言的模型有多种，GPT只是其中的一种。另一种很著名的模型是BERT 模型（后面会讲）。

GPT也不只是用在跟你“聊天”上的ChatGPT ，它还有更底层作为基座的InstructGPT 。

目前我们讨论的GPT一般指的是GPT-3以及它的升级版GPT-3.5，但GPT目前已经到了第四版，也就是GPT-4 。

GPT-3 是由人工智能公司OpenAl 训练与开发，该模型设计基于谷歌开发的变换语言模型（Transformer 模型，后面会提到）。OpenAI 于 2020 年 5 月发表了GPT-3 的论文，微软在 2020 年 9 月 22 日宣布取得了GPT-3 的独家授权。

所以，现在大家都说，微软赢麻了，谷歌慌得了，就是因为ChatGPT 微软的“势力范围”。

毕竟，如果所有人都找ChatGPT 问问题，而不在搜索引擎上搜索，谷歌的广告业务不就芭比Q了吗？

但，迟早人们可以用自然语言跟机器对话得到问题的答案。搜索引擎作为信息入口的功能，肯定会被既能直接提供答案，又能作为信息入口的GPT等新方式所取代。

## 生成式AI 和判别式AI

生成式AI ，就是帮你做东西的AI。判别式AI ，就是机器能够帮助辨别东西的AI，也叫决策式AI 。

比如，ChatGPT，在你提问之后说话给你巴拉巴拉一大堆，这就是生成式AI。你让一个作图AI，按照你提的要求做个画，这也是生成式AI。

生成式AI 为啥火，因为它能够直接响应人，直接跟人交流，这是人们最期待的AI 方式。就跟《星际穿越》里面的TARS 机器人一样。

判别式AI ，也挺重要的，典型的就是让机器具有像人一样的认识能力。比如，人工视觉、听音识曲、自动感知后自动判别然后再自动决策等。我们数字营销行业的营销自动化（MA ），就很可以利用上判别式AI 。比如，自主判别某个用户是否属于高机会型潜在客户，然后自动为他提供相应的商业信息或营销诱饵。

生成式AI 和判别式AI 没有孰优孰劣之分，它们是机器智能的两个必备能力。就如同人，既要有判断力，也要有创造力。既能有决定做不做一件事情的能力，也要有能够把事情做出来的能力。两种AI 就是对应的人的这两种能力。

## 语料

语言的材料。这个词并不是在ChatGPT 等自然语言AI产生之后才产生的。例如，我们学习一门外语，也需要语料。毕竟，没有人天生就懂一门自己从来没有见过的语言。

我时常在想，当中国人第一次接触英语的时候，是谁这么聪明，能第一个学会英语呢？肯定是一个“中外混血儿”，他的妈妈是老外，爸爸是中国人吧！

但事实上，并不需要这样，据说最早学会外语的中国人是学者，或者应该说，最早学会中文的外国人是学者（南怀仁、汤若望这些），然后他们又教会中国人学会外语。他们怎么学会的？就是基于生活在中国（或者外国），而拥有了丰富的语料资源。

语料的英语是Corpus，字典上的解释是：一套书面文本，特别是某一特定作者的全部作品或某一特定主题的写作。

在ChatGPT 等模型中，语料被分解为Token 和各种向量关系，通过预训练的方式，人们基于这些Token 和向量关系，建立起各种参数和模型，成为可被机器“消化、吸收”的原始学习素材。

所以，语料是ChatGPT的原材料，没有语料，就没有ChatGPT。

我这篇文章，未来就有很大可能成为某些自然语言人工智能模型的语料。

## Token

Token 是语言模型用于处理和生成文本的文本单位。我们通常认为，一个单词就是一个Token，但实际上并不如此，比如OpenAI 算两个token，分别是open和ai，再比如ChatGPT是chat 、g、p 和t 这四个token。

Token对ChatGPT至关重要，是ChatGPT理解和生成语言的最基本元素。

在用户输入一段话后，它使用一个分词算法将每个输入的单词拆分成token 。例如，“Hello world！”将被拆分为3个 token ：\[“Hello”，“world”，“！”\]。“I’m happy 😊”将被拆分为5个token：\[“I”, “’”, “m”, “happy”, “😊”\]。

简单讲，ChatGPT通过预先训练（预训练是无监督学习的方式，关于监督学习和无监督学习，我们后面介绍）生成了一个token 列表和不同token 之间的关系参数（后面也会介绍参数是什么），这些参数的数量极为庞大（以千亿甚至万亿计）。之后，则通过监督学习不断优化这些token 和参数。最终，能够让机器自主找到不同的语境下最合适的token 和参数，从而以最合适地方式把token 组合起来，形成人能够读懂的、合理的表达。

当然，这个过程很复杂，要用到近几年才出现的“Transformer ”模型，这个模型推动了ChatGPT的成功。

## 参数

参数这个东西，是人工智能中非常重要的一个概念，也是人工智能得以实现的非常重要的手段。

理解参数是什么，并不困难。首先，你需要理解，人工智能本质上仍然是“输入 – 计算 – 输出”的经典计算机模式。这个模式，从来没有发生过改变。改变的（或者更准确说，应该是进化的），主要是机器计算的能力越来越强大了。

那么，机器怎么能够越来越强大到，自己能够做出“充满智能”的计算，而不需要人去干预呢？

这就是参数在其中起到的作用。

你可以这么简单地去理解参数：

在人工智能下，计算，不再是人去直接写算式，而是让机器去自主地调节“计算公式”。这个“公式”，随着要解决的问题的复杂度的升高，里面包含的变量和常量就会越来越多。每个常量或者变量所占有的权重不一样，对它们赋予不同的权重，计算后输出的结果也就会非常不同。机器要做的事情，就是基于它输出的这些结果的正确与否（结果正确与否，通常是人告诉机器的，但在一些应用中，也可以不需要人），来调整这些权重，直到每一次计算出来的结果，都是正确或接近正确的。

这些权重，实际上，就是参数。

除了权重之外，还有支持向量机中的支持向量，以及线性回归或者逻辑回归中的系数，也都是参数。支持向量和回归是什么，就不再多做介绍了，感兴趣的朋友查一下度娘，内容很多。

人工智能的一个重要方法（但不是唯一方法），就是通过训练，不断让机器学会自主调整这些参数。

据说GPT-3有1750亿个参数。不过，据说，参数也不是越多越好。这些，我们就不深究了。

## LM

（大模型，Large Model ）

现在另一个非常火的概念是大模型。

我先讲讲模型。

模型，就是我们在前面讲“参数”的时候，所提到的“计算公式”。

计算公式能够适应不同的场景（语境）的一个原因，就是因为这些公式里面有可被不断动态调整的参数。当然，公式本身也是可调的，也不是一成不变的。你可以简单地人为，参数和公式，就组成了模型。基于不断增加的学习材料（比如语料和token ），以及不断告诉机器它所做出的结果的正确与否，机器就能不断迭代和优化参数和公式。这个过程也就是模型不断被训练的过程。

人工智能、机器学习、深度学习等等这些技术，背后都离不开模型。模型的好坏，一方面由最初算法的好坏决定，另一方面，也由学习训练过程的好坏决定。

我们把语料转成token ，目的也就是让机器能够基于这些素材，建立模型，并不断优化。

那么，什么是大模型呢？

其实，当你看了我前面讲的“参数”是指什么，大模型也就很容易理解，就是那些拥有很多参数的模型。

ChatGPT目前千亿级别的量级的参数，肯定是大模型。

国内的大厂，也都在做大模型，也是至少百亿级别量级的参数。它们的大模型也不是都跟ChatGPT一样只是用在自然语言上，更多的，是应用在了广告投放或者内容推荐上。当然，也可用在更广泛的领域。

## LLM大语言模型

（Large Language Model）

了解了大模型，就很容易理解LLM （大语言模型）了。

大模型中，专门用来理解、处理、生成自然语言的模型，就是大语言模型。

大语言模型的大，主要就是我们前面所说的参数量特别大。而参数量大，又必须以语料库和token 数量大为基础。

前面也有提到，ChatGPT和BERT都是典型的大语言模型。未来，肯定还会有更多的大语言模型出现。

现在看来，ChatGPT占得了先机，因为它更适合生成式任务。

谷歌的BERT（据说百度也是用的类似于BERT的模型）的原理是采用遮挡方法，就是把一句话的上下文遮挡住，然后让机器去“猜测”被遮挡的部分是什么，然后不断对猜测的结果进行反馈，以训练机器理解语言的能力。这种方式，让BERT更加适合于判别式任务。

举个例子，给BERT 一段文本和一个问题，让它在这个文本中找到这个问题的答案，或者判断这个文本的情绪倾向是更积极还是更消极。这些事情，BERT 很擅长。

不过，ChatGPT不是这么干的，它直接模仿人类从左到右的阅读，是一个“单向”的语言训练模型。所谓“单向”，就是从一句话（或者一段话）出发，去预测它后面应该接上什么话。所以，这就创造了我们今天跟ChatGPT对话的模式（常常看到它一本正经地“骗人”和胡说八道）。

另外，ChatGPT也好，还是BERT也好，都是基于我们前面几次提到的“Transformer 模型”。这个模型是目前最热的一个自然语言机器学习模型，利用了被称为“自注意力机制（Self-Attention Mechanism ）”的方法。这个模型对不从事人工智能研发的朋友来说，理解起来不是很简单，我就不多介绍了。你需要知道的是，为什么现在机器理解语言的速度变快了，背后就是这个模型的功劳，它可以让机器并行计算，大大提高了速度。

## Prompt和Prompt Engineering

Prompt的意思是提示。

Prompt Engineering的意思是提示工程。

有人说，这就是魔法师的“念咒”：对机器念咒，就输出给你你想要的东西。对，差不多。

Prompt 这个词在计算机科学中出现的很早，类似于你给机器下达的指令，但这个指令又不是程序命令，而更偏向于人类的自然语言。今天，在生成式AI 中，prompt 其实就是给机器提要求。它是自然语言的要求，不过最好能够精炼、言简意赅，并且清除表示你想要什么。

不过，任何提问，哪怕是对着人提问，把问题描述清楚，都是一个非常重要的前提。衡量一个人是否聪明的一个重要标志性指针，就是这个人的提问，是不是更高水平的。

对机器的提问或者指示，也需要有更高的水平，机器才能更好理解，并最终能够按照你的需要给你做出输出。

所以，如何提出更好的prompt 是要学习的。有点类似于，你要学会如何跟机器说话。有些人在淘宝上贩卖现成的prompts ，已经成为了一个商机。

Prompt engineering则是把给机器下达指令作为一个严肃的工程技术来进行研究。它所做的事情，不仅是让我们的指令更合理，更能帮助我们得到我们想要的回答。更是帮助挖掘人工智能的极限，以及找到人工智能的缺陷。

Prompt engineering有很多的方法，比如few-shot 方法、zero-shot 方法、CoT 方法等。我在后面介绍。